/**
 * Article Processor
 * 
 * Handles individual article processing through the LangChain agent.
 */

import { HumanMessage, SystemMessage } from '@langchain/core/messages';
import { NewsItem } from '../types';
import { AgentQueueItem, ToolCall } from './types';
import { NewsAgent } from './agent-factory';
import { SYSTEM_PROMPT, buildUserMessage } from './prompts';
import { parseAgentResponse, createFallbackPosts, forceGeneration, extractPosts, GeneratedPosts } from './parser';
import { getAbortController, isAborted } from './state';
import { updateQueueItem, markNewsItemProcessed, logActivity } from './store';

// ============= TYPES =============

export interface ProcessResult {
    skipped: boolean;
    generated: boolean;
    tierUsed: 1 | 2 | 3 | 4;
    toolCalls: number;
    reasoning?: string;
}

// ============= STREAM HANDLER =============

interface StreamResult {
    finalContent: string;
    toolsUsed: string[];
    toolCalls: ToolCall[];
}

/**
 * Stream agent response and track tool calls
 */
/**
 * Stream agent response and track tool calls
 */
async function streamAgentResponse(
    agent: NewsAgent,
    article: NewsItem,
    runId: string,
    modelId: string
): Promise<StreamResult> {
    const userMessage = buildUserMessage(article);
    const toolCalls: ToolCall[] = [];
    const toolsUsed: string[] = [];
    let finalContent = '';

    const stream = await agent.stream(
        {
            messages: [
                new SystemMessage(SYSTEM_PROMPT),
                new HumanMessage(userMessage),
            ],
            currentArticle: {
                id: article.id!,
                title: article.title,
                source: article.source_name || 'Unknown',
                url: article.link,
                snippet: article.content_snippet || '',
            },
            model: modelId,
        },
        {
            recursionLimit: 50, // Higher limit for loop
            signal: getAbortController()?.signal,
            configurable: {
                thread_id: runId,
            },
        }
    );

    for await (const event of stream) {
        if (isAborted()) {
            throw new Error('Aborted');
        }

        const evt = event as any;

        // Handle Planner
        if (evt.planner) {
            const plan = evt.planner.plan;
            console.log(`[Processor] Plan generated: ${plan?.steps.length} steps`);
            await logActivity(runId, 'thinking', `üìÖ Plan: ${plan?.steps.length} steps generated`, article.title);
        }

        // Handle Executor
        if (evt.executor) {
            const { currentStep, plan, context } = evt.executor;
            // The step that just finished is currentStep - 1 (since executor increments it)
            // But safely, let's look at the plan
            const lastStepIndex = currentStep - 1;
            if (plan && plan.steps[lastStepIndex]) {
                const step = plan.steps[lastStepIndex];
                console.log(`[Processor] Step ${lastStepIndex + 1} done: ${step.status}`);

                await logActivity(runId, step.status === 'completed' ? 'success' : 'error',
                    `Step ${lastStepIndex + 1}: ${step.instruction}`, article.title);

                if (step.tool && step.tool !== 'none') {
                    toolsUsed.push(step.tool);
                    toolCalls.push({
                        tool: step.tool as any,
                        input: step.instruction,
                        output: step.result,
                        timestamp: new Date().toISOString(),
                    });
                }
            }
        }

        // Handle Generator
        if (evt.generator) {
            const { generatedContent } = evt.generator;
            if (generatedContent) {
                // Construct the JSON expected by the parser
                finalContent = JSON.stringify({
                    decision: 'generate', // Generator always implies generation unless skipped
                    reasoning: "Generated by Deep Agent",
                    x_post: generatedContent.xPost,
                    instagram_caption: generatedContent.instagram,
                    facebook_post: generatedContent.facebook,
                    hashtags: generatedContent.hashtags,
                });
                console.log('[Processor] Content generated');
            }
        }

        // Handle Reflector
        if (evt.reflector) {
            const { reflections } = evt.reflector;
            if (reflections && reflections.length > 0) {
                const lastRef = reflections[reflections.length - 1];
                await logActivity(runId, 'thinking', `ü§î Reflection: Score ${lastRef.score}/10`, article.title);
            }
        }
    }

    return { finalContent, toolsUsed, toolCalls };
}

/**
 * Log tool call initiation
 */
async function logToolCall(runId: string, articleTitle: string, toolName: string): Promise<void> {
    if (toolName === 'read_article') {
        await logActivity(runId, 'reading', 'üìñ Step 2: Reading full article...', articleTitle, toolName);
    } else if (toolName === 'search_web' || toolName === 'search_news') {
        await logActivity(runId, 'searching', 'üîç Step 3: Searching for verification...', articleTitle, toolName);
    } else if (toolName === 'get_guidance' || toolName === 'find_similar') {
        await logActivity(runId, 'thinking', `üí° Consulting memory: ${toolName}`, articleTitle, toolName);
    } else {
        await logActivity(runId, 'tool', `üîß Using ${toolName}`, articleTitle, toolName);
    }
}

/**
 * Log tool execution result
 */
async function logToolResult(runId: string, articleTitle: string, toolName: string): Promise<void> {
    if (toolName === 'read_article') {
        await logActivity(runId, 'step', 'üìÑ Article content retrieved', articleTitle);
    } else if (toolName === 'search_web' || toolName === 'search_news') {
        await logActivity(runId, 'step', 'üîé Search results received', articleTitle);
    }
}

// ============= SAVE RESULTS =============

/**
 * Save generated posts to database
 */
async function saveResults(
    queueItem: AgentQueueItem,
    article: NewsItem,
    posts: GeneratedPosts,
    toolCalls: ToolCall[]
): Promise<void> {
    await updateQueueItem(queueItem.id!, {
        status: 'completed',
        decision: 'generate',
        reasoning: posts.reasoning,
        x_post: posts.xPost || undefined,
        instagram_caption: posts.instagram || undefined,
        facebook_post: posts.facebook || undefined,
        hashtags: posts.hashtags,
        tool_calls: toolCalls,
    });

    await markNewsItemProcessed(
        article.id!,
        posts.xPost || undefined,
        posts.instagram || undefined,
        posts.facebook || undefined
    );
}

// ============= MAIN PROCESSOR =============

/**
 * Process a single article from the queue
 */
export async function processArticle(
    article: NewsItem,
    queueItem: AgentQueueItem,
    runId: string,
    agent: NewsAgent,
    modelId: string = 'moonshotai/kimi-k2-instruct-0905'
): Promise<ProcessResult> {
    const toolCallsArray: ToolCall[] = [];

    try {
        // Step 1: Analyzing
        await logActivity(runId, 'thinking', 'üß† Step 1: Analyzing snippet...', article.title);
        console.log(`[Agent] Processing: ${article.title.slice(0, 50)}...`);

        // Step 2: Stream agent response
        const { finalContent, toolCalls } = await streamAgentResponse(agent, article, runId, modelId);
        toolCallsArray.push(...toolCalls);

        // Step 3: Parse response
        await logActivity(runId, 'generating', '‚úçÔ∏è Step 4: Making decision...', article.title);
        const parsed = parseAgentResponse(finalContent);

        let posts: GeneratedPosts;

        if (!parsed) {
            // Parse failed - create fallback
            console.log(`[Agent] Parse error, creating fallback posts for: ${article.title.slice(0, 30)}...`);
            posts = createFallbackPosts(article, toolCallsArray);
        } else if (parsed.decision === 'skip' || !parsed.x_post) {
            // LLM tried to skip - force generation
            console.log(`[Agent] LLM tried to skip, forcing generation for: ${article.title.slice(0, 30)}...`);
            posts = forceGeneration(article, parsed, toolCallsArray);
        } else {
            // Normal generation
            await logActivity(runId, 'generating', `üìù Generating posts for X, Instagram, Facebook...`, article.title);
            posts = extractPosts(parsed, toolCallsArray);
        }

        // Step 4: Save results
        await saveResults(queueItem, article, posts, toolCallsArray);

        return {
            skipped: false,
            generated: true,
            tierUsed: posts.tierUsed,
            toolCalls: toolCallsArray.length,
            reasoning: posts.reasoning,
        };

    } catch (error) {
        await updateQueueItem(queueItem.id!, {
            status: 'failed',
            error: error instanceof Error ? error.message : 'Unknown',
            tool_calls: toolCallsArray,
        });
        throw error;
    }
}
